# Curvine åˆ†å¸ƒå¼ç¼“å­˜ç³»ç»Ÿä½¿ç”¨æŒ‡å—

[![Version](https://img.shields.io/badge/version-v1.0-blue.svg)](https://github.com/curvine/curvine)
[![License](https://img.shields.io/badge/license-Apache%202.0-green.svg)](https://www.apache.org/licenses/LICENSE-2.0)
[![Documentation](https://img.shields.io/badge/docs-latest-brightgreen.svg)](https://docs.curvine.io)

## ğŸ“š ç›®å½•

- [ğŸ¯ ç³»ç»Ÿæ¦‚è¿°](#-ç³»ç»Ÿæ¦‚è¿°)
- [ğŸ“‚ è·¯å¾„æŒ‚è½½ç®¡ç†](#-è·¯å¾„æŒ‚è½½ç®¡ç†)
- [ğŸ’¾ æ™ºèƒ½ç¼“å­˜ç­–ç•¥](#-æ™ºèƒ½ç¼“å­˜ç­–ç•¥)
- [ğŸ”„ æ•°æ®ä¸€è‡´æ€§ä¿éšœ](#-æ•°æ®ä¸€è‡´æ€§ä¿éšœ)
- [ğŸ¤– AI/ML åœºæ™¯åº”ç”¨](#-aiml-åœºæ™¯åº”ç”¨)
- [ğŸ—„ï¸ å¤§æ•°æ®ç”Ÿæ€é›†æˆ](#-å¤§æ•°æ®ç”Ÿæ€é›†æˆ)
- [ğŸ’¡ æœ€ä½³å®è·µ](#-æœ€ä½³å®è·µ)
- [ğŸ¯ æ€»ç»“](#-æ€»ç»“)

---

## ğŸ¯ ç³»ç»Ÿæ¦‚è¿°

Curvine æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½ã€äº‘åŸç”Ÿçš„åˆ†å¸ƒå¼ç¼“å­˜ç³»ç»Ÿï¼Œä¸“ä¸ºç°ä»£æ•°æ®å¯†é›†å‹åº”ç”¨è®¾è®¡ã€‚å®ƒåœ¨åº•å±‚å­˜å‚¨ï¼ˆUFSï¼‰å’Œè®¡ç®—å¼•æ“ä¹‹é—´æä¾›ä¸€ä¸ªæ™ºèƒ½ç¼“å­˜å±‚ï¼Œæ˜¾è‘—æå‡æ•°æ®è®¿é—®æ€§èƒ½ã€‚


### ğŸ† æ€§èƒ½ä¼˜åŠ¿

ç›¸æ¯”ä¼ ç»Ÿå­˜å‚¨è®¿é—®æ–¹å¼ï¼ŒCurvine å¯ä»¥æä¾›ï¼š

| æŒ‡æ ‡ | äº‘å­˜å‚¨ | Curvine ç¼“å­˜ | æ€§èƒ½æå‡ |
|------|----------|--------------|----------|
| **è¯»å–å»¶è¿Ÿ** | 100-500ms | 1-10ms | **10-50x** |
| **ååé‡** | 100-500 MB/s | 1-10 GB/s | **10-20x** |
| **IOPS** | 1K-10K | 100K-1M | **100x** |
| **å¹¶å‘è¿æ¥** | 100-1K | 10K-100K | **100x** |

---

### æ ¸å¿ƒç»„ä»¶

- **Master é›†ç¾¤**: å…ƒæ•°æ®ç®¡ç†ã€ç¼“å­˜è°ƒåº¦ã€ä¸€è‡´æ€§ä¿éšœ
- **Worker èŠ‚ç‚¹**: æ•°æ®ç¼“å­˜ã€I/O å¤„ç†ã€ä»»åŠ¡æ‰§è¡Œ
- **Client SDK**: å¤šè¯­è¨€å®¢æˆ·ç«¯ï¼Œæ”¯æŒRustã€Fuseã€Javaã€Python
- **Job Manager**: åˆ†å¸ƒå¼ä»»åŠ¡è°ƒåº¦å’Œç®¡ç†
- **Metrics ç³»ç»Ÿ**: å®æ—¶ç›‘æ§å’Œæ€§èƒ½åˆ†æ

---


## ğŸ“‚ è·¯å¾„æŒ‚è½½ç®¡ç†

æŒ‚è½½æ˜¯ä½¿ç”¨ Curvine ç¼“å­˜çš„ç¬¬ä¸€æ­¥ï¼Œå®ƒå»ºç«‹äº†åº•å±‚å­˜å‚¨ï¼ˆUFSï¼‰ä¸ç¼“å­˜è·¯å¾„çš„æ˜ å°„å…³ç³»ã€‚

### æŒ‚è½½æ¨¡å¼è¯¦è§£

Curvine æ”¯æŒä¸¤ç§çµæ´»çš„æŒ‚è½½æ¨¡å¼ï¼š

#### ğŸ¯ CST æ¨¡å¼ï¼ˆä¸€è‡´è·¯å¾„æ¨¡å¼ï¼‰
```bash
# è·¯å¾„ä¿æŒä¸€è‡´ï¼Œä¾¿äºç®¡ç†å’Œç»´æŠ¤
bin/cv mount s3://bucket/data /bucket/data --mnt-type cst
```

**é€‚ç”¨åœºæ™¯**: 
- è·¯å¾„ç»“æ„æ¸…æ™°çš„æ•°æ®æ¹–åœºæ™¯
- éœ€è¦ç›´è§‚è·¯å¾„æ˜ å°„çš„ç”Ÿäº§ç¯å¢ƒ
- å¤šå›¢é˜Ÿåä½œçš„æ•°æ®å¹³å°

#### ğŸ”€ Arch æ¨¡å¼ï¼ˆç¼–æ’æ¨¡å¼ï¼‰
```bash
# çµæ´»è·¯å¾„æ˜ å°„ï¼Œæ”¯æŒå¤æ‚çš„è·¯å¾„å˜æ¢
bin/cv mount s3://complex-bucket/deep/nested/path /simple/data --mnt-type arch
```

**é€‚ç”¨åœºæ™¯**:
- å¤æ‚çš„å­˜å‚¨å±‚æ¬¡ç»“æ„
- éœ€è¦è·¯å¾„æŠ½è±¡çš„åœºæ™¯
- å¤šäº‘å­˜å‚¨ç»Ÿä¸€è®¿é—®

### å®Œæ•´æŒ‚è½½ç¤ºä¾‹

```bash
# æŒ‚è½½ S3 å­˜å‚¨åˆ° Curvineï¼ˆç”Ÿäº§çº§é…ç½®ï¼‰
bin/cv mount \
s3://bucket/warehouse/tpch_500g.db/orders \
/bucket/warehouse/tpch_500g.db/orders \
--ttl-ms 24h \
--ttl-action delete \
--replicas 3 \
--block-size 128MB \
--consistency-strategy always \
--storage-type ssd \
-c s3.endpoint_url=https://s3.ap-southeast-1.amazonaws.com \
-c s3.credentials.access=access_key \
-c s3.credentials.secret=secret_key \
-c s3.region_name=ap-southeast-1 
```

### æŒ‚è½½å‚æ•°è¯¦è§£

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|--------|------|------|
| `--ttl-ms` | duration | `0` | ç¼“å­˜æ•°æ®è¿‡æœŸæ—¶é—´ | `24h`, `7d`, `30d` |
| `--ttl-action` | enum | `none` | è¿‡æœŸç­–ç•¥ï¼š`delete`/`none` | `delete` |
| `--replicas` | int | `1` | æ•°æ®å‰¯æœ¬æ•°ï¼ˆ1-5ï¼‰ | `3` |
| `--block-size` | size | `128MB` | ç¼“å­˜å—å¤§å° | `64MB`, `128MB`, `256MB` |
| `--consistency-strategy` | enum | `always` | ä¸€è‡´æ€§ç­–ç•¥ | `none`/`always`/`period` |
| `--storage-type` | enum | `disk` | å­˜å‚¨ä»‹è´¨ç±»å‹ | `mem`/`ssd`/`disk` |

### æŒ‚è½½ç‚¹ç®¡ç†

```bash
# æŸ¥çœ‹æ‰€æœ‰æŒ‚è½½ç‚¹
bin/cv mount

# å¸è½½è·¯å¾„
bin/cv unmount /bucket/warehouse/tpch_500g.db/orders
```

---

## ğŸ’¾ æ™ºèƒ½ç¼“å­˜ç­–ç•¥

Curvine æä¾›å¤šç§æ™ºèƒ½ç¼“å­˜ç­–ç•¥ï¼Œä»è¢«åŠ¨å“åº”åˆ°ä¸»åŠ¨é¢„æµ‹ï¼Œå…¨æ–¹ä½ä¼˜åŒ–æ•°æ®è®¿é—®æ€§èƒ½ã€‚

### ä¸»åŠ¨æ•°æ®é¢„åŠ è½½

ä¸»åŠ¨åŠ è½½è®©æ‚¨å¯ä»¥åœ¨ä¸šåŠ¡é«˜å³°å‰é¢„çƒ­ç¼“å­˜ï¼Œç¡®ä¿æœ€ä½³æ€§èƒ½ï¼š

```bash
# åŸºç¡€åŠ è½½
bin/cv load s3:/bucket/warehouse/critical-dataset

# å¸¦è¿›åº¦ç›‘æ§çš„åŒæ­¥åŠ è½½
bin/cv load s3:/bucket/warehouse/critical-dataset -w

```

### è‡ªåŠ¨ç¼“å­˜ç­–ç•¥

Curvine çš„è‡ªåŠ¨ç¼“å­˜ç³»ç»Ÿç›¸æ¯”ä¼ ç»Ÿæ–¹æ¡ˆå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼š

#### âœ¨ Curvine æ™ºèƒ½ç¼“å­˜æ¶æ„

![curvine](./curvine.png)

#### æ ¸å¿ƒä¼˜åŠ¿å¯¹æ¯”

| ç‰¹æ€§ | å¼€æºç«å“ | Curvine | ä¼˜åŠ¿è¯´æ˜ |
|------|----------|---------|----------|
| **åŠ è½½ç²’åº¦** | Blockçº§åˆ« | File/Directoryçº§åˆ« | é¿å…ç¢ç‰‡åŒ–ï¼Œä¿è¯å®Œæ•´æ€§ |
| **é‡å¤å¤„ç†** | å­˜åœ¨é‡å¤åŠ è½½ | æ™ºèƒ½å»é‡ | èŠ‚çœå¸¦å®½å’Œå­˜å‚¨èµ„æº |
| **ä»»åŠ¡è°ƒåº¦** | ç®€å•é˜Ÿåˆ— | åˆ†å¸ƒå¼Job Manager | é«˜æ•ˆå¹¶å‘ï¼Œè´Ÿè½½å‡è¡¡ |
| **ä¸€è‡´æ€§ä¿éšœ** | è¢«åŠ¨æ£€æŸ¥ | ä¸»åŠ¨æ„ŸçŸ¥ | å®æ—¶æ•°æ®åŒæ­¥ |

---

## ğŸ”„ æ•°æ®ä¸€è‡´æ€§ä¿éšœ

æ•°æ®ä¸€è‡´æ€§æ˜¯ç¼“å­˜ç³»ç»Ÿçš„æ ¸å¿ƒæŒ‘æˆ˜ï¼ŒCurvine æä¾›å¤šå±‚æ¬¡çš„ä¸€è‡´æ€§ä¿éšœæœºåˆ¶ã€‚

### ä¸€è‡´æ€§ç­–ç•¥è¯¦è§£

#### 1. ğŸš« None æ¨¡å¼ - æœ€é«˜æ€§èƒ½
```bash
bin/cv mount s3://bucket/path /bucket/path --consistency-strategy=none
```
- **é€‚ç”¨åœºæ™¯**: é™æ€æ•°æ®ã€å½’æ¡£æ•°æ®ã€åªè¯»æ•°æ®é›†
- **æ€§èƒ½**: â­â­â­â­â­ (æœ€å¿«)
- **ä¸€è‡´æ€§**: â­â­ (ä¾èµ–TTL)

#### 2. âœ… Always æ¨¡å¼ - å¼ºä¸€è‡´æ€§
```bash
bin/cv mount s3://bucket/path /bucket/path --consistency-strategy=always
```
- **é€‚ç”¨åœºæ™¯**: ç»å¸¸æ›´æ–°çš„ä¸šåŠ¡æ•°æ®ã€å…³é”®ä¸šåŠ¡ç³»ç»Ÿ
- **æ€§èƒ½**: â­â­â­ (æœ‰å¼€é”€)
- **ä¸€è‡´æ€§**: â­â­â­â­â­ (å¼ºä¸€è‡´æ€§)

#### 3. ğŸ•°ï¸ Period æ¨¡å¼ - å¹³è¡¡æ–¹æ¡ˆ
```bash
bin/cv mount s3://bucket/path /bucket/path \
  --consistency-strategy=period \
  --check-interval=5m
```
- **é€‚ç”¨åœºæ™¯**: æ›´æ–°é¢‘ç‡å¯é¢„æœŸçš„æ•°æ®
- **æ€§èƒ½**: â­â­â­â­ (è¾ƒå¥½)
- **ä¸€è‡´æ€§**: â­â­â­â­ (å®šæœŸä¿è¯)

### ç¼“å­˜æ€§èƒ½ç›‘æ§

ç›‘æ§ç¼“å­˜å‘½ä¸­ç‡æ˜¯è¯„ä¼°ä¸€è‡´æ€§ç­–ç•¥æ•ˆæœçš„é‡è¦æ‰‹æ®µï¼š

```bash
# è·å–ç¼“å­˜å‘½ä¸­ç‡
curl -s http://master:9001/metrics | grep -E "(cache_hits|cache_misses)"
```

```prometheus
client_mount_cache_hits{id="3108497238"} 823307
client_mount_cache_misses{id="3108497238"} 4380
```

---

## ğŸ¤– AI/ML åœºæ™¯åº”ç”¨

AI å’Œæœºå™¨å­¦ä¹ å·¥ä½œè´Ÿè½½å¯¹å­˜å‚¨æ€§èƒ½æœ‰æé«˜è¦æ±‚ï¼ŒCurvine ä¸ºæ­¤æä¾›äº†ä¸“é—¨ä¼˜åŒ–çš„åŠŸèƒ½ã€‚

### æ·±åº¦å­¦ä¹ è®­ç»ƒä¼˜åŒ–

```bash
# ä¸º GPU é›†ç¾¤ä¼˜åŒ–çš„æ•°æ®åŠ è½½
bin/cv mount s3://datasets/imagenet /datasets/imagenet \
  --storage-type=mem \
  --block-size=1GB \
  --replicas=2 \

```

### æ¨¡å‹æœåŠ¡åœºæ™¯

```bash
# æ¨¡å‹æ–‡ä»¶ç¼“å­˜ï¼ˆä½å»¶è¿Ÿè®¿é—®ï¼‰
bin/cv mount s3://model/bert-large /models/bert-large \
  --storage-type=mem \
  --ttl-ms=none \
  --consistency-strategy=always \

# æ¨ç†æ•°æ®ç¼“å­˜
bin/cv mount s3://inference/input /inference/input \
  --storage-type=ssd \
  --ttl-ms=1h \
  --consistency-strategy=none 
```


### ğŸ”— POSIX è¯­ä¹‰ä¸ FUSE è®¿é—®

Curvine å®Œç¾æ”¯æŒ POSIX è¯­ä¹‰ï¼Œé€šè¿‡ FUSEï¼ˆFilesystem in Userspaceï¼‰æ¥å£ï¼Œå¯ä»¥å°† Curvine é›†ç¾¤æŒ‚è½½ä¸ºæœ¬åœ°æ–‡ä»¶ç³»ç»Ÿï¼Œä¸º AI/ML åº”ç”¨æä¾›é€æ˜çš„æ–‡ä»¶è®¿é—®ä½“éªŒã€‚


#### AI/ML è®­ç»ƒä¸­çš„ FUSE ä½¿ç”¨

##### 1. æ·±åº¦å­¦ä¹ è®­ç»ƒæ•°æ®è®¿é—®

```python
# PyTorch è®­ç»ƒè„šæœ¬
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import os

class CurvineImageDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        """
        é€šè¿‡ FUSE æŒ‚è½½ç‚¹ç›´æ¥è®¿é—® Curvine ä¸­çš„æ•°æ®
        root_dir: FUSE æŒ‚è½½ç‚¹è·¯å¾„ï¼Œå¦‚ /curvine-fuse/datasets/imagenet
        """
        self.root_dir = root_dir
        self.transform = transform
        self.image_paths = []
        
        # ç›´æ¥éå† FUSE æŒ‚è½½çš„ç›®å½•
        for class_dir in os.listdir(root_dir):
            class_path = os.path.join(root_dir, class_dir)
            if os.path.isdir(class_path):
                for img_file in os.listdir(class_path):
                    if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):
                        self.image_paths.append(os.path.join(class_path, img_file))
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        # é€šè¿‡æ ‡å‡†æ–‡ä»¶æ“ä½œè®¿é—®æ•°æ®ï¼Œäº«å— Curvine ç¼“å­˜åŠ é€Ÿ
        img_path = self.image_paths[idx]
        image = Image.open(img_path).convert('RGB')
        
        if self.transform:
            image = self.transform(image)
            
        # ä»è·¯å¾„æå–æ ‡ç­¾
        label = os.path.basename(os.path.dirname(img_path))
        return image, label

# ä½¿ç”¨ç¤ºä¾‹
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                        std=[0.229, 0.224, 0.225])
])

# ç›´æ¥ä½¿ç”¨ FUSE æŒ‚è½½ç‚¹çš„è·¯å¾„
dataset = CurvineImageDataset(
    root_dir='/curvine-fuse/datasets/imagenet/train',
    transform=transform
)

dataloader = DataLoader(
    dataset, 
    batch_size=64, 
    shuffle=True, 
    num_workers=8,
    pin_memory=True
)

# è®­ç»ƒå¾ªç¯
for epoch in range(num_epochs):
    for batch_idx, (data, targets) in enumerate(dataloader):
        # æ•°æ®é€šè¿‡ FUSE è‡ªåŠ¨ä» Curvine ç¼“å­˜åŠ è½½
        # äº«å—æ¥è¿‘å†…å­˜çš„è®¿é—®é€Ÿåº¦
        outputs = model(data.cuda())
        loss = criterion(outputs, targets.cuda())
        # ... è®­ç»ƒé€»è¾‘
```

##### 2. TensorFlow/Keras æ•°æ®ç®¡é“

```python
import tensorflow as tf
import os

def create_curvine_dataset(data_dir, batch_size=32):
    """
    é€šè¿‡ FUSE æŒ‚è½½ç‚¹åˆ›å»º TensorFlow æ•°æ®ç®¡é“
    data_dir: FUSE æŒ‚è½½çš„æ•°æ®ç›®å½•
    """
    
    # ç›´æ¥ä½¿ç”¨æ ‡å‡†æ–‡ä»¶ API è®¿é—® FUSE æŒ‚è½½çš„æ•°æ®
    def load_and_preprocess_image(path):
        # TensorFlow é€šè¿‡ FUSE é€æ˜è®¿é—® Curvine ç¼“å­˜
        image = tf.io.read_file(path)
        image = tf.image.decode_jpeg(image, channels=3)
        image = tf.image.resize(image, [224, 224])
        image = tf.cast(image, tf.float32) / 255.0
        return image
    
    # æ‰«æ FUSE æŒ‚è½½ç›®å½•ä¸­çš„æ–‡ä»¶
    image_paths = []
    labels = []
    
    for class_name in os.listdir(data_dir):
        class_dir = os.path.join(data_dir, class_name)
        if os.path.isdir(class_dir):
            for img_file in os.listdir(class_dir):
                if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):
                    image_paths.append(os.path.join(class_dir, img_file))
                    labels.append(class_name)
    
    # åˆ›å»ºæ•°æ®é›†
    path_ds = tf.data.Dataset.from_tensor_slices(image_paths)
    label_ds = tf.data.Dataset.from_tensor_slices(labels)
    
    # åº”ç”¨é¢„å¤„ç†
    image_ds = path_ds.map(
        load_and_preprocess_image, 
        num_parallel_calls=tf.data.AUTOTUNE
    )
    
    # ç»„åˆæ•°æ®å’Œæ ‡ç­¾
    dataset = tf.data.Dataset.zip((image_ds, label_ds))
    
    return dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)

# ä½¿ç”¨ç¤ºä¾‹
train_dataset = create_curvine_dataset('/curvine-fuse/datasets/imagenet/train')
val_dataset = create_curvine_dataset('/curvine-fuse/datasets/imagenet/val')

# æ¨¡å‹è®­ç»ƒ
model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=50,
    callbacks=[
        tf.keras.callbacks.ModelCheckpoint('/curvine-fuse/models/checkpoints/'),
        tf.keras.callbacks.TensorBoard(log_dir='/curvine-fuse/logs/')
    ]
)
```

---

## ğŸ—„ï¸ å¤§æ•°æ®ç”Ÿæ€é›†æˆ

Curvine ä¸ä¸»æµå¤§æ•°æ®æ¡†æ¶æ— ç¼é›†æˆï¼Œæä¾›é€æ˜çš„ç¼“å­˜åŠ é€Ÿèƒ½åŠ›ã€‚

### Hadoop ç”Ÿæ€é›†æˆ

#### åŸºç¡€é…ç½®

åœ¨ `hdfs-site.xml` æˆ– `core-site.xml` ä¸­æ·»åŠ ï¼š

```xml
<!-- Curvine FileSystem å®ç° -->
<property>
    <name>fs.cv.impl</name>
    <value>io.curvine.CurvineFileSystem</value>
</property>

<!-- å•é›†ç¾¤é…ç½® -->
<property>
    <name>fs.cv.master_addrs</name>
    <value>master1:8995,master2:8995,master3:8995</value>
</property>

```

#### å¤šé›†ç¾¤æ”¯æŒ

```xml
<!-- é›†ç¾¤1ï¼šç”Ÿäº§ç¯å¢ƒ -->
<property>
    <name>fs.cv.production.master_addrs</name>
    <value>prod-master1:8995,prod-master2:8995,prod-master3:8995</value>
</property>

<!-- é›†ç¾¤2ï¼šå¼€å‘ç¯å¢ƒ -->
<property>
    <name>fs.cv.development.master_addrs</name>
    <value>dev-master1:8995,dev-master2:8995</value>
</property>

<!-- é›†ç¾¤3ï¼šæœºå™¨å­¦ä¹ ä¸“ç”¨é›†ç¾¤ -->
<property>
    <name>fs.cv.ml-cluster.master_addrs</name>
    <value>ml-master1:8995,ml-master2:8995,ml-master3:8995</value>
</property>
```


### ğŸ”„ UFSé€æ˜ä»£ç†

ä¸ºäº†æ›´å¥½åœ°æ”¯æŒç°æœ‰Javaåº”ç”¨æ— ç¼æ¥å…¥Curvineç¼“å­˜ï¼Œæˆ‘ä»¬æä¾›äº†UFSé€æ˜ä»£ç†è§£å†³æ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆçš„æ ¸å¿ƒä¼˜åŠ¿æ˜¯**é›¶ä»£ç ä¿®æ”¹**ï¼Œè®©ç°æœ‰åº”ç”¨å¯ä»¥ç«‹å³äº«å—Curvineçš„ç¼“å­˜åŠ é€Ÿæ•ˆæœã€‚

#### âœ¨ é€æ˜ä»£ç†æ ¸å¿ƒç‰¹æ€§

- **ğŸš« é›¶ä»£ç ä¿®æ”¹**: ä¿ç•™åŸæœ‰æ‰€æœ‰æ¥å£ä¸å˜ï¼Œä¸šåŠ¡ä»£ç æ— éœ€ä»»ä½•ä¿®æ”¹
- **ğŸ” æ™ºèƒ½è·¯å¾„è¯†åˆ«**: ä»…åœ¨æ–‡ä»¶æ‰“å¼€æ—¶åˆ¤æ–­è·¯å¾„æ˜¯å¦å·²æŒ‚è½½åˆ°Curvine
- **âš¡ è‡ªåŠ¨ç¼“å­˜åŠ é€Ÿ**: å·²æŒ‚è½½è·¯å¾„è‡ªåŠ¨å¯ç”¨ç¼“å­˜åŠ é€Ÿï¼ŒæœªæŒ‚è½½è·¯å¾„èµ°åŸç”ŸS3è®¿é—®
- **ğŸ”„ å¹³æ»‘åˆ‡æ¢**: æ”¯æŒåœ¨è¿è¡Œæ—¶åŠ¨æ€åˆ‡æ¢æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼Œæ— éœ€é‡å¯åº”ç”¨

#### ğŸ› ï¸ é…ç½®æ–¹å¼

åªéœ€è¦æ›¿æ¢Hadoopé…ç½®ä¸­çš„S3FileSystemå®ç°ç±»ï¼š

```xml
<!-- ä¼ ç»ŸS3è®¿é—®é…ç½® -->
<!--
<property>
    <name>fs.s3a.impl</name>
    <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>
</property>
-->

<!-- æ›¿æ¢ä¸ºCurvineé€æ˜ä»£ç† -->
<property>
    <name>fs.s3a.impl</name>
    <value>io.curvine.S3AProxyFileSystem</value>
</property>

<property>
    <name>fs.cv.impl</name>
    <value>io.curvine.CurvineFileSystem</value>
</property>

<!-- Curvineé›†ç¾¤é…ç½® -->
<property>
    <name>fs.curvine.master_addrs</name>
    <value>master1:8995,master2:8995,master3:8995</value>
</property>

```

#### ğŸ”§ å·¥ä½œåŸç†

![Working Principle](./WorkingPrinciple.png)

#### ğŸš€ ä½¿ç”¨ç¤ºä¾‹

**æ— éœ€ä¿®æ”¹ä»»ä½•ä¸šåŠ¡ä»£ç ï¼ŒåŸæœ‰ä»£ç ç›´æ¥äº«å—åŠ é€Ÿï¼š**

```java
// ä¸šåŠ¡ä»£ç å®Œå…¨ä¸å˜ï¼
Configuration conf = new Configuration();
FileSystem fs = FileSystem.get(URI.create("s3a://my-bucket/"), conf);

// è¿™ä¸ªè·¯å¾„å¦‚æœå·²æŒ‚è½½åˆ°Curvineï¼Œè‡ªåŠ¨äº«å—ç¼“å­˜åŠ é€Ÿ
FSDataInputStream input = fs.open(new Path("s3a://my-bucket/warehouse/data.parquet"));

// è¿™ä¸ªè·¯å¾„å¦‚æœæœªæŒ‚è½½ï¼Œèµ°åŸç”ŸS3è®¿é—®
FSDataInputStream input2 = fs.open(new Path("s3a://my-bucket/archive/old-data.parquet"));
```

**Spark/MapReduceä»£ç ç¤ºä¾‹ï¼š**

```java
// Sparkä»£ç æ— éœ€ä»»ä½•ä¿®æ”¹
Dataset<Row> df = spark.read()
    .option("header", "true")
    // å¦‚æœ/warehouse/è·¯å¾„å·²æŒ‚è½½ï¼Œè‡ªåŠ¨ä½¿ç”¨ç¼“å­˜åŠ é€Ÿ
    .csv("s3a://data-lake/warehouse/customer_data/");
    
df.groupBy("region")
  .agg(sum("revenue").alias("total_revenue"))
  .orderBy(desc("total_revenue"))
  .show(20);
```

**Python PySparkç¤ºä¾‹ï¼š**

```python
# Pythonä»£ç ä¹Ÿæ— éœ€ä¿®æ”¹
from pyspark.sql import SparkSession
from pyspark.sql.functions import sum, desc

spark = SparkSession.builder.appName("TransparentCache").getOrCreate()

# è‡ªåŠ¨åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ç¼“å­˜
df = spark.read \
    .option("header", "true") \
    .csv("s3a://data-lake/analytics/events/")

result = df.groupBy("event_type") \
    .agg(sum("count").alias("total_events")) \
    .orderBy(desc("total_events"))
    
result.show()
```

### Apache Spark ä¼˜åŒ–é…ç½®

```bash
# Spark åº”ç”¨å¯åŠ¨é…ç½®
spark-submit \
  --class com.example.SparkApp \
  --master yarn \
  --deploy-mode cluster \
  --conf spark.hadoop.fs.cv.impl=io.curvine.CurvineFileSystem \
  --conf spark.hadoop.fs.cv.master_addrs=master1:8995,master2:8995,master3:8995 \
  --conf spark.sql.adaptive.enabled=true \
  --jars curvine-hadoop-client.jar \
  app.jar
```

#### Spark ä»£ç ç¤ºä¾‹

```scala
// Scala ç¤ºä¾‹
val spark = SparkSession.builder()
  .appName("Curvine Demo")
  .config("spark.hadoop.fs.cv.impl", "io.curvine.CurvineFileSystem")
  .getOrCreate()

// ç›´æ¥ä½¿ç”¨ cv:// åè®®è®¿é—®ç¼“å­˜æ•°æ®
val df = spark.read
  .option("multiline", "true")
  .json("cv://production/warehouse/events/2024/01/01/")

df.groupBy("event_type")
  .count()
  .show()

// å¤šé›†ç¾¤è®¿é—®
val prodData = spark.read.parquet("cv://production/warehouse/sales/")
val mlData = spark.read.parquet("cv://ml-cluster/features/user_profiles/")
```

```python
# Python ç¤ºä¾‹
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("Curvine Python Demo") \
    .config("spark.hadoop.fs.cv.impl", "io.curvine.CurvineFileSystem") \
    .config("spark.hadoop.fs.cv.master_addrs", "master1:8995,master2:8995") \
    .getOrCreate()

# è¯»å–ç¼“å­˜ä¸­çš„æ•°æ®
df = spark.read.option("header", "true") \
    .csv("cv://warehouse/customer_data/")

# å¤æ‚æŸ¥è¯¢è‡ªåŠ¨äº«å—ç¼“å­˜åŠ é€Ÿ
result = df.groupBy("region") \
    .agg({"revenue": "sum", "orders": "count"}) \
    .orderBy("sum(revenue)", ascending=False)

result.show(20)
```

### Trino/Presto æ’ä»¶é›†æˆ

Curvine æä¾›äº†æ™ºèƒ½è·¯å¾„æ›¿æ¢æ’ä»¶ï¼Œå¯ä»¥åšåˆ°æ— ä¾µå…¥æ€§å®ç°ç¼“å­˜åŠ é€Ÿï¼Œä¸éœ€è¦ä¸šåŠ¡ä¿®æ”¹ä»£ç ï¼Œå®ç°å®Œå…¨é€æ˜çš„ç¼“å­˜åŠ é€Ÿï¼š

#### æ’ä»¶å·¥ä½œæµç¨‹

![Plugin Workflow](./PluginWorkflow.png)

spark æ’ä»¶ä½¿ç”¨å®ä¾‹ï¼š
```
spark-submit \
--class main.scala.Tpch \
--name tpch_demo \
--conf spark.hadoop.fs.cv.impl=io.curvine.CurvineFileSystem \
--conf spark.hadoop.fs.cv.default.master_addrs=master1:8995,master2:8995 \
--conf spark.sql.extensions=io.curvine.spark.CurvineSparkExtension \
```

### Flink å®æ—¶è®¡ç®—é›†æˆ

```java
// Flink Table API é›†æˆç¤ºä¾‹
TableEnvironment tableEnv = TableEnvironment.create(settings);

// é…ç½® Curvine FileSystem
Configuration config = new Configuration();
config.setString("fs.cv.impl", "io.curvine.CurvineFileSystem");
config.setString("fs.cv.master_addrs", "master1:8995,master2:8995");

// åˆ›å»º Curvine è¡¨
tableEnv.executeSql(
    "CREATE TABLE user_events (" +
    "  user_id BIGINT," +
    "  event_type STRING," +
    "  timestamp_ms BIGINT," +
    "  properties MAP<STRING, STRING>" +
    ") WITH (" +
    "  'connector' = 'filesystem'," +
    "  'path' = 'cv://streaming/events/'," +
    "  'format' = 'json'" +
    ")"
);

// å®æ—¶æŸ¥è¯¢äº«å—ç¼“å­˜åŠ é€Ÿ
Table result = tableEnv.sqlQuery(
    "SELECT user_id, COUNT(*) as event_count " +
    "FROM user_events " +
    "WHERE timestamp_ms > UNIX_TIMESTAMP() * 1000 - 3600000 " +
    "GROUP BY user_id"
);
```

---


---

## ğŸ’¡ æœ€ä½³å®è·µ

### ğŸ¯ æŒ‚è½½ç­–ç•¥æœ€ä½³å®è·µ

#### æŒ‰ä¸šåŠ¡åœºæ™¯åˆ†å±‚æŒ‚è½½

```bash
# çƒ­æ•°æ®ï¼šé«˜é¢‘è®¿é—®ï¼Œä½¿ç”¨å†…å­˜ç¼“å­˜
bin/cv mount s3://bucket/hot /bucket/hot \
  --storage-type=mem \
  --replicas=3 \
  --ttl-ms=1d \
  --ttl-action=delete

# æ¸©æ•°æ®ï¼šå®šæœŸè®¿é—®ï¼Œä½¿ç”¨ SSD ç¼“å­˜
bin/cv mount s3://bucket/warm /bucket/warm \
  --storage-type=ssd \
  --replicas=2 \
  --ttl-ms=7d \
  --ttl-action=delete


# å†·æ•°æ®ï¼šä½é¢‘è®¿é—®ï¼Œä½¿ç”¨ç£ç›˜ç¼“å­˜
bin/cv mount s3://bucket/cold /bucket/cold \
  --storage-type=disk \
  --replicas=1 \
  --ttl-ms=30d \
  --ttl-action=delete
```

#### æŒ‰æ•°æ®ç±»å‹ä¼˜åŒ–

```bash
# å°æ–‡ä»¶å¯†é›†å‹ï¼ˆå¦‚æ—¥å¿—ã€é…ç½®ï¼‰
bin/cv mount s3://bucket/logs /bucket/logs \
  --block-size=4MB \
  --consistency-strategy=none \

# å¤§æ–‡ä»¶å‹ï¼ˆå¦‚è§†é¢‘ã€æ¨¡å‹ï¼‰
bin/cv mount s3://bucket/models /bucket/models \
  --block-size=1GB \
  --consistency-strategy=always \

# åˆ†æå‹æ•°æ®ï¼ˆå¦‚ Parquetï¼‰
bin/cv mount s3://bucket/analytics /bucket/analytics \
  --block-size=128MB \
  --consistency-strategy=none \
```
---

## ğŸ¯ æ€»ç»“

Curvine ä½œä¸ºæ–°ä¸€ä»£åˆ†å¸ƒå¼ç¼“å­˜ç³»ç»Ÿï¼Œé€šè¿‡æ™ºèƒ½ç¼“å­˜ç­–ç•¥ã€å¼ºä¸€è‡´æ€§ä¿éšœå’Œæ— ç¼ç”Ÿæ€é›†æˆï¼Œä¸ºç°ä»£æ•°æ®å¯†é›†å‹åº”ç”¨æä¾›äº†å“è¶Šçš„æ€§èƒ½æå‡ã€‚

### ğŸ† æ ¸å¿ƒä»·å€¼

- **ğŸš€ æ€§èƒ½æå‡**: 10-100å€çš„è®¿é—®åŠ é€Ÿï¼Œæ˜¾è‘—é™ä½æ•°æ®è®¿é—®å»¶è¿Ÿ
- **ğŸ’° æˆæœ¬ä¼˜åŒ–**: å‡å°‘äº‘å­˜å‚¨è®¿é—®è´¹ç”¨ï¼Œæé«˜è®¡ç®—èµ„æºåˆ©ç”¨ç‡  
- **ğŸ›¡ï¸ æ•°æ®å®‰å…¨**: å¤šé‡ä¸€è‡´æ€§ä¿éšœï¼Œç¡®ä¿æ•°æ®å‡†ç¡®æ€§å’Œå®Œæ•´æ€§
- **ğŸŒ ç”Ÿæ€å‹å¥½**: ä¸ä¸»æµå¤§æ•°æ®å’Œ AI æ¡†æ¶æ— ç¼é›†æˆ

---

*Curvine - è®©æ•°æ®è®¿é—®å¿«å¦‚é—ªç”µ âš¡*
